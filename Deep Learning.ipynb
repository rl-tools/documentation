{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e2340b-bd7f-444c-b170-3cf25395df3d",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a2835f-e821-4026-b901-f75413a07372",
   "metadata": {},
   "source": [
    "Because of the static multiple dispatch paradigm layed out in [Multiple Dispatch.ipynb](./Multiple%20Dispatch.ipynb), we need to first include the primitive operations for the device(s) we are inteding on using such that the algorithms (and datastructures) we later include for deep learning can use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0b4658-4f41-4893-8680-710851f9ddf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#include <backprop_tools/operations/cpu.h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2094df96-089b-4e95-acee-780a1c8ddc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <backprop_tools/nn/layers/dense/operations_cpu.h>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78774374-12bf-44e5-8e1c-a3818eb4adc5",
   "metadata": {},
   "source": [
    "We set up the environment as described in [Containers.ipynb](./Containers.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ecf6cc5-5d1e-4d00-b259-bcf73603aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace bpt = backprop_tools;\n",
    "using DEVICE = bpt::devices::DefaultCPU;\n",
    "using T = float;\n",
    "using TI = typename DEVICE::index_t;\n",
    "DEVICE device;\n",
    "TI seed = 1;\n",
    "auto rng = bpt::random::default_engine(DEVICE::SPEC::RANDOM(), seed);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f2c9b-0ddc-4e07-bcd0-73b0ca876083",
   "metadata": {},
   "source": [
    "As justified by our analysis of the reinforcement learnign for continuous control landscape (in the [paper](https://arxiv.org/abs/2306.03530)) in the beginning **BackpropTools** only supports fully connected neural networks. But we are planning on adding more architectures (especially recurrent neural networks) in the future.\n",
    "\n",
    "We can instantiate a simple layer by first defining its hyperparameters (which are compile-time `constexpr` and types):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa34ef39-917e-420b-818b-09ed88d61ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "constexpr TI INPUT_DIM = 5;\n",
    "constexpr TI OUTPUT_DIM = 5;\n",
    "constexpr auto ACTIVATION_FUNCTION = bpt::nn::activation_functions::RELU;\n",
    "using PARAMETER_TYPE = bpt::nn::parameters::Plain;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88fd32-7ac5-41da-84c9-51b6a6301420",
   "metadata": {},
   "source": [
    "We will explain the role of the `PARAMETER_TYPE` later on. \n",
    "\n",
    "These hyperparameters and other options are combined into a specification type such that it is easier to pass it around and such that we don't need to write out all hyperparameters and options as template parameters when a function takes the datastructure as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5fcc23f-cd17-4a09-ba45-5a63d05eb296",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LAYER_SPEC = bpt::nn::layers::dense::Specification<T, TI, INPUT_DIM, OUTPUT_DIM, ACTIVATION_FUNCTION, PARAMETER_TYPE>;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5f9638-5b70-4081-87d9-9ea4057035ff",
   "metadata": {},
   "source": [
    "Using this specification we can declare an actual layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8b7f82-d285-420d-8a6e-6285cfe9df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpt::nn::layers::dense::Layer<LAYER_SPEC> layer;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9646be16-4863-4cdc-99bb-1383838f5b9c",
   "metadata": {},
   "source": [
    "A fully connected neural network consists of layers each implementing: $$y = f(Wx + b)$$ where $x$ is the input (external or from the previous layer), $W$ and $b$ are the weight matrix and biases respectively and $f$ is an element-wise non-linear function. Hence the data structure of a layer should contain at least $W$ and $b$. Because these parameters are containers they need to be allocated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ca6a927-c634-4421-8c9e-b999007427ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpt::malloc(device, layer);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb20f2d-3ccf-44b6-8bb0-7aa3347e7cb5",
   "metadata": {},
   "source": [
    "Now that the memory is allocated we need to initialize it (because it may contain arbitrary values). We use the standard [Kaiming](https://pytorch.org/docs/stable/nn.init.html?highlight=kaiming#torch.nn.init.kaiming_normal_) initialization scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c95939-dbd0-45de-8ae1-c80235a96e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpt::init_kaiming(device, layer, rng);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ee612-947e-4b19-9fde-0af5b2c4621f",
   "metadata": {},
   "source": [
    "We can print $W$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7991b1-d05d-4806-8e2a-bf44b133230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -0.329563     0.228620    -0.036984     0.029308    -0.251371 \n",
      "    0.159981     0.160368     0.388801    -0.104199     0.017367 \n",
      "   -0.416291    -0.399396     0.026565     0.153081    -0.440328 \n",
      "   -0.387428    -0.073803     0.167055     0.079583     0.384994 \n",
      "    0.024086    -0.364958     0.137669    -0.075132     0.179950 \n"
     ]
    }
   ],
   "source": [
    "bpt::print(device, layer.weights.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238c12bc-6f2e-4369-8185-4ebd0f80797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -0.447207    -0.405136     0.296024    -0.104276     0.309621 \n"
     ]
    }
   ],
   "source": [
    "bpt::print(device, layer.biases.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1efca3-dac7-4349-b1ec-29303a5e5187",
   "metadata": {},
   "source": [
    "Now that the layer is initialized we can run inference using a random input. We first declare and allocate input and output matrices and then randomly initialize the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f75aa99-574c-4824-8465-28ec4196c4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.175199    -0.863064     1.316539     0.942564    -0.589718 \n"
     ]
    }
   ],
   "source": [
    "constexpr TI BATCH_SIZE = 1;\n",
    "bpt::MatrixDynamic<bpt::matrix::Specification<T, TI, BATCH_SIZE, INPUT_DIM>> input;\n",
    "bpt::MatrixDynamic<bpt::matrix::Specification<T, TI, BATCH_SIZE, INPUT_DIM>> output;\n",
    "bpt::malloc(device, input);\n",
    "bpt::malloc(device, output);\n",
    "bpt::randn(device, input, rng);\n",
    "bpt::print(device, input);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ec43d4-ce8c-4f95-a3d2-4f2343d1f8d9",
   "metadata": {},
   "source": [
    "Now we can evaluate output of the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44fd2759-3415-4ae0-92eb-b87c6e9b992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.000000     0.000000     1.006727     0.000000     0.633133 \n"
     ]
    }
   ],
   "source": [
    "bpt::evaluate(device, layer, input, output);\n",
    "bpt::print(device, output);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc931a9d-80c4-4e81-9467-6e23e2330a7b",
   "metadata": {},
   "source": [
    "Now we are revisiting the `PARAMETER_TYPE` template argument. \n",
    "For inference storing $W$ and $b$ is sufficient but for training we at least need to also store the gradient of the loss $L$ wrt. $W$ and $b$: $\\frac{\\mathrm{d}L}{\\mathrm{d}W}$ and $\\frac{\\mathrm{d}L}{\\mathrm{d}b}$. Because depending on the optimizer type we might need to store more information per parameter (like the first and second-order moment in the case of [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)), we abstract the storage for the weights and biases using a `PARAMETER_TYPE` that can e.b. be `Plain`, `Gradient`, `Adam` or any other type extended by the user. For this illustration we are using `Gradient`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bad47751-686d-4a73-9cce-58e08b2f731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PARAMETER_TYPE_2 = bpt::nn::parameters::Gradient;\n",
    "using LAYER_2_SPEC = bpt::nn::layers::dense::Specification<T, TI, INPUT_DIM, OUTPUT_DIM, ACTIVATION_FUNCTION, PARAMETER_TYPE_2>;\n",
    "bpt::nn::layers::dense::LayerBackwardGradient<LAYER_2_SPEC> layer_2;\n",
    "bpt::malloc(device, layer_2);\n",
    "bpt::copy(device, device, layer_2, layer);\n",
    "bpt::zero_gradient(device, layer_2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f798c2d1-0108-41a9-8c11-2da61c261d06",
   "metadata": {},
   "source": [
    "Note that we now use the `bpt::nn::layers::dense::LayerBackwardGradient` datastructure which is supported by the functions implementing the backpropagation algorithm. Additionally, similar to PyTorch we are setting the gradient to zero because it is accumulated with subsequent backward passes.\n",
    "\n",
    "Now we can backpropagate the derivative of the loss wrt. the `output` to calculate the derivative of the loss wrt. the `input`. Hence the derivative of the loss wrt. the `output`: `d_output` is actually an input to the `bpt::backward` operator. The operator also accumulates the derivative of the loss wrt. the weights and biases in the layer. We first allocate containers for `d_input` and `d_output` and randomly set `d_output` (a hypothetical gradient of the input of some upstream layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f5ed38f-b72b-4e1d-81cc-092a4b830661",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpt::MatrixDynamic<bpt::matrix::Specification<T, TI, BATCH_SIZE, OUTPUT_DIM>> d_output;\n",
    "bpt::MatrixDynamic<bpt::matrix::Specification<T, TI, BATCH_SIZE, INPUT_DIM>> d_input;\n",
    "bpt::malloc(device, d_input);\n",
    "bpt::malloc(device, d_output);\n",
    "bpt::randn(device, d_output, rng);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d1952-38c1-41a1-93dc-a514522349ff",
   "metadata": {},
   "source": [
    "Now we execute the backpropagation and display the gradient of the loss wrt. the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "480022a0-51f9-4960-9b0e-7d4ad75aa83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -0.099916     0.307348    -0.325702     0.127334    -0.196570 \n"
     ]
    }
   ],
   "source": [
    "bpt::backward(device, layer_2, input, d_output, d_input);\n",
    "bpt::print(device, d_input);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2198f28c-4a49-4e86-bbd5-8cfac69f8974",
   "metadata": {},
   "source": [
    "This also accumulates the gradient in the weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d2ab5a0-46cc-4e4a-bb2f-023fd34df8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.000000     0.000000     0.000000     0.000000     0.000000 \n",
      "   -0.081795     0.402941    -0.614657    -0.440058     0.275324 \n",
      "    0.000000     0.000000     0.000000     0.000000     0.000000 \n",
      "    0.000000     0.000000     0.000000     0.000000     0.000000 \n",
      "   -0.183485     0.903885    -1.378809    -0.987145     0.617611 \n"
     ]
    }
   ],
   "source": [
    "bpt::print(device, layer_2.weights.gradient);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6cf8127-5d53-4c28-9475-d2a51daa2858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.000000    -0.466873     0.000000     0.000000    -1.047298 \n"
     ]
    }
   ],
   "source": [
    "bpt::print(device, layer_2.biases.gradient);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41003240-bc51-4a3c-bfa3-10344e0303c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpt::free(device, layer);\n",
    "bpt::free(device, layer_2);\n",
    "bpt::free(device, input);\n",
    "bpt::free(device, output);\n",
    "bpt::free(device, d_input);\n",
    "bpt::free(device, d_output);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca82c3d3-635c-4d06-9469-194af18d39cc",
   "metadata": {},
   "source": [
    "Until now we showed the behavior of a single, fully-connected layer. **BackpropTools** contains an [Multilayer Perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron) that conveniently integrates an arbitrary number of layers into a single data structure with algorithms to perform forward passes and backpropagation across the whole model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1919412-7ebb-4ac3-945d-d945ead54224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
