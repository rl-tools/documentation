{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59e2340b-bd7f-444c-b170-3cf25395df3d",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/rl-tools/documentation/binder?labpath=03-Deep%20Learning.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86a2835f-e821-4026-b901-f75413a07372",
   "metadata": {},
   "source": [
    "Because of the static multiple dispatch paradigm layed out in [Multiple Dispatch](./02-Multiple%20Dispatch.ipynb), we need to first include the primitive operations for the device(s) we are inteding on using such that the algorithms (and datastructures) we later include for deep learning can use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0b4658-4f41-4893-8680-710851f9ddf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#include <rl_tools/operations/cpu.h>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2094df96-089b-4e95-acee-780a1c8ddc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <rl_tools/nn/layers/dense/operations_cpu.h>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78774374-12bf-44e5-8e1c-a3818eb4adc5",
   "metadata": {},
   "source": [
    "We set up the environment as described in [Containers](./01-Containers.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ecf6cc5-5d1e-4d00-b259-bcf73603aa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "namespace rlt = rl_tools;\n",
    "using DEVICE = rlt::devices::DefaultCPU;\n",
    "using T = float;\n",
    "using TI = typename DEVICE::index_t;\n",
    "DEVICE device;\n",
    "TI seed = 1;\n",
    "auto rng = rlt::random::default_engine(DEVICE::SPEC::RANDOM(), seed);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0b9f2c9b-0ddc-4e07-bcd0-73b0ca876083",
   "metadata": {},
   "source": [
    "As justified by our analysis of the reinforcement learnign for continuous control landscape (in the [paper](https://arxiv.org/abs/2306.03530)) in the beginning **RLtools** only supports fully connected neural networks. But we are planning on adding more architectures (especially recurrent neural networks) in the future.\n",
    "\n",
    "We can instantiate a simple layer by first defining its hyperparameters (which are compile-time `constexpr` and types):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa34ef39-917e-420b-818b-09ed88d61ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "constexpr TI INPUT_DIM = 5;\n",
    "constexpr TI OUTPUT_DIM = 5;\n",
    "constexpr auto ACTIVATION_FUNCTION = rlt::nn::activation_functions::RELU;\n",
    "using PARAMETER_TYPE = rlt::nn::parameters::Plain;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c88fd32-7ac5-41da-84c9-51b6a6301420",
   "metadata": {},
   "source": [
    "We will explain the role of the `PARAMETER_TYPE` later on. \n",
    "\n",
    "These hyperparameters and other options are combined into a specification type such that it is easier to pass it around and such that we don't need to write out all hyperparameters and options as template parameters when a function takes the datastructure as an argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5fcc23f-cd17-4a09-ba45-5a63d05eb296",
   "metadata": {},
   "outputs": [],
   "source": [
    "using LAYER_SPEC = rlt::nn::layers::dense::Specification<T, TI, INPUT_DIM, OUTPUT_DIM, ACTIVATION_FUNCTION, PARAMETER_TYPE>;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a5f9638-5b70-4081-87d9-9ea4057035ff",
   "metadata": {},
   "source": [
    "Using this specification we can declare an actual layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea8b7f82-d285-420d-8a6e-6285cfe9df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlt::nn::layers::dense::Layer<LAYER_SPEC> layer;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9646be16-4863-4cdc-99bb-1383838f5b9c",
   "metadata": {},
   "source": [
    "A fully connected neural network consists of layers each implementing: $$y = f(Wx + b)$$ where $x$ is the input (external or from the previous layer), $W$ and $b$ are the weight matrix and biases respectively and $f$ is an element-wise non-linear function. Hence the data structure of a layer should contain at least $W$ and $b$. Because these parameters are containers they need to be allocated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ca6a927-c634-4421-8c9e-b999007427ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlt::malloc(device, layer);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bb20f2d-3ccf-44b6-8bb0-7aa3347e7cb5",
   "metadata": {},
   "source": [
    "Now that the memory is allocated we need to initialize it (because it may contain arbitrary values). We use the standard [Kaiming](https://pytorch.org/docs/stable/nn.init.html?highlight=kaiming#torch.nn.init.kaiming_normal_) initialization scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c95939-dbd0-45de-8ae1-c80235a96e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlt::init_kaiming(device, layer, rng);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e74ee612-947e-4b19-9fde-0af5b2c4621f",
   "metadata": {},
   "source": [
    "We can print $W$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd7991b1-d05d-4806-8e2a-bf44b133230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -0.329563     0.228620    -0.036984     0.029308    -0.251371 \n",
      "    0.159981     0.160368     0.388801    -0.104199     0.017367 \n",
      "   -0.416291    -0.399396     0.026565     0.153081    -0.440328 \n",
      "   -0.387428    -0.073803     0.167055     0.079583     0.384994 \n",
      "    0.024086    -0.364958     0.137669    -0.075132     0.179950 \n"
     ]
    }
   ],
   "source": [
    "rlt::print(device, layer.weights.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "238c12bc-6f2e-4369-8185-4ebd0f80797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -0.447207    -0.405136     0.296024    -0.104276     0.309621 \n"
     ]
    }
   ],
   "source": [
    "rlt::print(device, layer.biases.parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8b1efca3-dac7-4349-b1ec-29303a5e5187",
   "metadata": {},
   "source": [
    "Now that the layer is initialized we can run inference using a random input. We first declare and allocate input and output matrices and then randomly initialize the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f75aa99-574c-4824-8465-28ec4196c4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.175199    -0.863064     1.316539     0.942564    -0.589718 \n"
     ]
    }
   ],
   "source": [
    "constexpr TI BATCH_SIZE = 1;\n",
    "rlt::MatrixDynamic<rlt::matrix::Specification<T, TI, BATCH_SIZE, INPUT_DIM>> input;\n",
    "rlt::MatrixDynamic<rlt::matrix::Specification<T, TI, BATCH_SIZE, OUTPUT_DIM>> output;\n",
    "rlt::malloc(device, input);\n",
    "rlt::malloc(device, output);\n",
    "rlt::randn(device, input, rng);\n",
    "rlt::print(device, input);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1ec43d4-ce8c-4f95-a3d2-4f2343d1f8d9",
   "metadata": {},
   "source": [
    "Now we can evaluate output of the layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44fd2759-3415-4ae0-92eb-b87c6e9b992f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.000000     0.000000     1.006727     0.000000     0.633133 \n"
     ]
    }
   ],
   "source": [
    "rlt::evaluate(device, layer, input, output);\n",
    "rlt::print(device, output);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc931a9d-80c4-4e81-9467-6e23e2330a7b",
   "metadata": {},
   "source": [
    "Now we are revisiting the `PARAMETER_TYPE` template argument. \n",
    "For inference storing $W$ and $b$ is sufficient but for training we at least need to also store the gradient of the loss $L$ wrt. $W$ and $b$: $\\frac{\\mathrm{d}L}{\\mathrm{d}W}$ and $\\frac{\\mathrm{d}L}{\\mathrm{d}b}$. Because depending on the optimizer type we might need to store more information per parameter (like the first and second-order moment in the case of [Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam)), we abstract the storage for the weights and biases using a `PARAMETER_TYPE` that can e.b. be `Plain`, `Gradient`, `Adam` or any other type extended by the user. For this illustration we are using `Gradient`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bad47751-686d-4a73-9cce-58e08b2f731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "using PARAMETER_TYPE_2 = rlt::nn::parameters::Gradient;\n",
    "using LAYER_2_SPEC = rlt::nn::layers::dense::Specification<T, TI, INPUT_DIM, OUTPUT_DIM, ACTIVATION_FUNCTION, PARAMETER_TYPE_2>;\n",
    "rlt::nn::layers::dense::LayerBackwardGradient<LAYER_2_SPEC> layer_2;\n",
    "rlt::malloc(device, layer_2);\n",
    "rlt::copy(device, device, layer_2, layer);\n",
    "rlt::zero_gradient(device, layer_2);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f798c2d1-0108-41a9-8c11-2da61c261d06",
   "metadata": {},
   "source": [
    "Note that we now use the `rlt::nn::layers::dense::LayerBackwardGradient` datastructure which is supported by the functions implementing the backpropagation algorithm. Additionally, similar to PyTorch we are setting the gradient to zero because it is accumulated with subsequent backward passes.\n",
    "\n",
    "Now we can backpropagate the derivative of the loss wrt. the `output` to calculate the derivative of the loss wrt. the `input`. Hence the derivative of the loss wrt. the `output`: `d_output` is actually an input to the `rlt::backward` operator. The operator also accumulates the derivative of the loss wrt. the weights and biases in the layer. We first allocate containers for `d_input` and `d_output` and randomly set `d_output` (a hypothetical gradient of the input of some upstream layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f5ed38f-b72b-4e1d-81cc-092a4b830661",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlt::MatrixDynamic<rlt::matrix::Specification<T, TI, BATCH_SIZE, OUTPUT_DIM>> d_output;\n",
    "rlt::MatrixDynamic<rlt::matrix::Specification<T, TI, BATCH_SIZE, INPUT_DIM>> d_input;\n",
    "rlt::malloc(device, d_input);\n",
    "rlt::malloc(device, d_output);\n",
    "rlt::randn(device, d_output, rng);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "260d1952-38c1-41a1-93dc-a514522349ff",
   "metadata": {},
   "source": [
    "Now we execute the backpropagation and display the gradient of the loss wrt. the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "480022a0-51f9-4960-9b0e-7d4ad75aa83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -0.383490     0.504066    -0.357525     0.152552    -0.412863 \n"
     ]
    }
   ],
   "source": [
    "rlt::backward(device, layer_2, input, d_output, d_input);\n",
    "rlt::print(device, d_input);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2198f28c-4a49-4e86-bbd5-8cfac69f8974",
   "metadata": {},
   "source": [
    "This also accumulates the gradient in the weights and biases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d2ab5a0-46cc-4e4a-bb2f-023fd34df8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.150751    -0.742629     1.132824     0.811034    -0.507426 \n",
      "   -0.081795     0.402941    -0.614657    -0.440058     0.275324 \n",
      "    0.000000     0.000000     0.000000     0.000000     0.000000 \n",
      "    0.000000     0.000000     0.000000     0.000000     0.000000 \n",
      "   -0.183485     0.903885    -1.378809    -0.987145     0.617611 \n"
     ]
    }
   ],
   "source": [
    "rlt::print(device, layer_2.weights.gradient);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6cf8127-5d53-4c28-9475-d2a51daa2858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0.860456    -0.466873     0.000000     0.000000    -1.047298 \n"
     ]
    }
   ],
   "source": [
    "rlt::print(device, layer_2.biases.gradient);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41003240-bc51-4a3c-bfa3-10344e0303c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "// rlt::free(device, layer);\n",
    "// rlt::free(device, layer_2);\n",
    "// rlt::free(device, input);\n",
    "// rlt::free(device, output);\n",
    "// rlt::free(device, d_input);\n",
    "// rlt::free(device, d_output);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca82c3d3-635c-4d06-9469-194af18d39cc",
   "metadata": {},
   "source": [
    "Until now we showed the behavior of a single, fully-connected layer. **RLtools** contains an [Multilayer Perceptron (MLP)](https://en.wikipedia.org/wiki/Multilayer_perceptron) that conveniently integrates an arbitrary number of layers into a single data structure with algorithms to perform forward passes and backpropagation across the whole model. The MLP is locate under the namespace `rl_tools::nn_models` hence we include its CPU operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1919412-7ebb-4ac3-945d-d945ead54224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <rl_tools/nn_models/operations_cpu.h>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1178b1da-9d39-4fe9-adf8-ecd55ec5280d",
   "metadata": {},
   "source": [
    "Next we define the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5cac540-074b-4b43-8963-6b6361f22474",
   "metadata": {},
   "outputs": [],
   "source": [
    "constexpr TI INPUT_DIM_MLP = 5;\n",
    "constexpr TI OUTPUT_DIM_MLP = 1;\n",
    "constexpr TI NUM_LAYERS = 3;\n",
    "constexpr TI HIDDEN_DIM = 10;\n",
    "constexpr auto ACTIVATION_FUNCTION_MLP = rlt::nn::activation_functions::RELU;\n",
    "constexpr auto OUTPUT_ACTIVATION_FUNCTION_MLP = rlt::nn::activation_functions::IDENTITY;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b04c9794-f234-418c-bc91-4a8311664db4",
   "metadata": {},
   "source": [
    "Note that the MLP supports architectures with an arbitrary depth but each layer has to have the same dimensionality. This is because the layers are stored in an array and hence all need to have the same type. If we would allow for different hidden dimensions, we would have to give up on having arbitrary depths. \n",
    "\n",
    "We aggregate the hyperparameters into a specification again (first just for the structure, later for the full network, incorporating the structure):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e537671-53bc-4de1-bc37-0b709bb78a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "using STRUCTURE_SPEC = rlt::nn_models::mlp::StructureSpecification<T, DEVICE::index_t, INPUT_DIM_MLP, OUTPUT_DIM_MLP, NUM_LAYERS, HIDDEN_DIM, ACTIVATION_FUNCTION_MLP, OUTPUT_ACTIVATION_FUNCTION_MLP, BATCH_SIZE>;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c61ac86b-1cdb-4b94-a68b-23db38e7386f",
   "metadata": {},
   "source": [
    "We use the default Adam parameters (taken from TensorFlow) and set up the optimizer type using these parameters. Moreover, we create a full network specification for a network that can be trained with Adam which takes the structure specification as an input. Finally we define the full network type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e89b3030-9bf7-4263-8cc0-8e2cd28883fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "using OPTIMIZER_PARAMETERS = rlt::nn::optimizers::adam::DefaultParametersTF<T, TI>;\n",
    "using OPTIMIZER = rlt::nn::optimizers::Adam<OPTIMIZER_PARAMETERS>;\n",
    "using MODEL_SPEC = rlt::nn_models::mlp::AdamSpecification<STRUCTURE_SPEC>;\n",
    "using MODEL_TYPE = rlt::nn_models::mlp::NeuralNetworkAdam<MODEL_SPEC>;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "41007542-5cfc-474b-87c3-64fdb0636aec",
   "metadata": {},
   "source": [
    "Using these type definitions we can now declare the optimizer and the model. All the optimizer state is contained in the `PARAMETER_TYPE` of the model (and an additional `age` integer in the model in the case of Adam). In comparison to PyTorch which stores the optimizer state in the optimizer, we prefer to store the first and second-order moment next to the parameters like it is the case for the gradient anyways (in PyTorch as well). Hence the optimizer is stateless in this case (does not need to be for user-defined optimizers) and we only need to allocate the model.\n",
    "\n",
    "The backpropagation algorithm needs to store the intermediate gradients. To save memory we do not add a `d_input` or `d_output` to each layer but rather use a double buffer with the maximum size of the hidden representation needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c07d799-8447-423e-a347-a55935e5d8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER optimizer;\n",
    "MODEL_TYPE model;\n",
    "typename MODEL_TYPE::Buffers<BATCH_SIZE> buffers;"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf615947-981d-4eb4-a434-b5ca6341268b",
   "metadata": {},
   "source": [
    "We allocate the model and set initialize its weights randomly like in the case for the single layer. We are again zeroing the gradient of all parameters of all layers as well as resetting the optimizer state of all parameters of all layers (e.g. in the case of Adam the first and second order moments are set to zero). Finally we also allocate the buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7afdb190-6cab-4ee9-ad13-f2aa8f0ea378",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlt::malloc(device, model);\n",
    "rlt::init_weights(device, model, rng); // recursively initializes all layers using kaiming initialization\n",
    "rlt::zero_gradient(device, model); // recursively zeros all gradients in the layers\n",
    "rlt::reset_optimizer_state(device, optimizer, model);\n",
    "rlt::malloc(device, buffers);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6099056-a9fa-400a-be14-08345cfe3e9f",
   "metadata": {},
   "source": [
    "In this example we showcase an MLP with a five dimensional input and a one dimensional output (remember the `OUTPUT_ACTIVATION_FUNCTION_MLP` is `IDENTITY` so it can also output negative values). For these new shapes we declare and allocate the input and output containers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7e5cf51-9565-4838-b7e9-7447d30a5c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlt::MatrixDynamic<rlt::matrix::Specification<T, TI, BATCH_SIZE, INPUT_DIM_MLP>> input_mlp, d_input_mlp;\n",
    "rlt::MatrixDynamic<rlt::matrix::Specification<T, TI, BATCH_SIZE, OUTPUT_DIM_MLP>> d_output_mlp;\n",
    "rlt::malloc(device, input_mlp);\n",
    "rlt::malloc(device, d_input_mlp);\n",
    "rlt::malloc(device, d_output_mlp);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfb8d31e-81dd-4b68-8904-4e273751e532",
   "metadata": {},
   "source": [
    "Now, like in the case of the single layer, we can run a forward pass using the input. Because the model is a Adam model (which is a subclass of `rlt::nn_models::mlp::NeuralNetworkBackwardGradient`), it stores the intermediate (and final) outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "142b17c6-acf9-484d-9857-286b0a486f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.128710f"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlt::randn(device, input_mlp, rng);\n",
    "rlt::forward(device, model, input_mlp);\n",
    "T output_value = get(model.output_layer.output, 0, 0);\n",
    "output_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a4a3357-e0ff-44ba-9156-a3a3b44d7962",
   "metadata": {},
   "source": [
    "Now imagine we want the output of the model (for this input) to be $1$. We calculate the error and feed it back through the model using backpropagation. `d_output_mlp` should be the derivative of the loss function, hence it gives the direction of the output that would increase the loss. Our error is the opposite, if we would move the output into the direction of the error we would come closer to our target value and hence decrease the loss. Because of this, we feed back `-error`. This procedure also corresponds to using a squared loss because `error` is (up to a constant) the derivative of the squared loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4afb8ea0-e3a9-427a-865a-7955789ba283",
   "metadata": {},
   "outputs": [],
   "source": [
    "T target_output_value = 1;\n",
    "T error = target_output_value - output_value;\n",
    "rlt::set(d_output_mlp, 0, 0, -error);\n",
    "rlt::backward(device, model, input_mlp, d_output_mlp, d_input_mlp, buffers);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "827ffdc4-af9d-43d0-b9aa-aef03ca9b7df",
   "metadata": {},
   "source": [
    "The backward pass populates the gradient in all parameters of the model. Using this gradient we can apply the `rlt::step` operator which updates the first and second order moments of the gradient of all parameters and afterwards applies the Adam update rule to update the parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d9c41f4-1dbb-4165-934f-d2b602abb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlt::step(device, optimizer, model);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "958781a9-5e08-466d-aed2-9099f7de8e39",
   "metadata": {},
   "source": [
    "Now the next forward pass should be closer to the target value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2d906129-b67f-45c2-af9c-8e1cea90cb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.139336f"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlt::forward(device, model, input_mlp);\n",
    "get(model.output_layer.output, 0, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59d21d7e-16a9-4f46-94b7-90a8c52469b6",
   "metadata": {},
   "source": [
    "Next we will train the network to actually perform a function (not only trying to output a constant value as before). With the following training loop we train it to behave like the `rlt::max` operator which outputs the max of the five inputs. We run the forward and backward pass for $32$ iterations while accumulating the gradient which effectively leads to a batch size of $32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97955ba6-c3a6-4245-9256-90e30c44c97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squared error: 1.693208\n",
      "Squared error: 0.081613\n",
      "Squared error: 0.018353\n",
      "Squared error: 0.004015\n",
      "Squared error: 0.014354\n",
      "Squared error: 0.017132\n",
      "Squared error: 0.003833\n",
      "Squared error: 0.014308\n",
      "Squared error: 0.002764\n",
      "Squared error: 0.003838\n"
     ]
    }
   ],
   "source": [
    "for(TI i=0; i < 10000; i++){\n",
    "    rlt::zero_gradient(device, model);\n",
    "    T mse = 0;\n",
    "    for(TI batch_i=0; batch_i < 32; batch_i++){\n",
    "        rlt::randn(device, input_mlp, rng);\n",
    "        rlt::forward(device, model, input_mlp);\n",
    "        T output_value = get(model.output_layer.output, 0, 0);\n",
    "        T target_output_value = rlt::max(device, input_mlp);\n",
    "        T error = target_output_value - output_value;\n",
    "        rlt::set(d_output_mlp, 0, 0, -error);\n",
    "        rlt::backward(device, model, input_mlp, d_output_mlp, d_input_mlp, buffers);\n",
    "        mse += error * error;\n",
    "    }\n",
    "    rlt::step(device, optimizer, model);\n",
    "    if(i % 1000 == 0)\n",
    "    std::cout << \"Squared error: \" << mse/32 << std::endl;\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5416b21-8dfa-4a50-a6a3-4bd160b74d9f",
   "metadata": {},
   "source": [
    "Now we can test the model using some arbitrary input (which should be in the distribution of input values) and the model should output a value close to the maximum of the five input values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2907576e-b957-4ce2-aaba-1ea1da46b1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.458271f"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(input_mlp, 0, 0, +0.0);\n",
    "set(input_mlp, 0, 1, -0.1);\n",
    "set(input_mlp, 0, 2, +0.5);\n",
    "set(input_mlp, 0, 3, -0.4);\n",
    "set(input_mlp, 0, 4, +0.1);\n",
    "\n",
    "rlt::forward(device, model, input_mlp);\n",
    "rlt::get(model.output_layer.output, 0, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b3479ea-06b6-4e52-b5d5-1bfc29ef6188",
   "metadata": {},
   "source": [
    "We can also automatically test it with $10$ random inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e086ec9b-a48b-412f-86ef-510b915f2e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.439409 output: 0.358949\n",
      "max: 1.676039 output: 1.596896\n",
      "max: 0.937396 output: 0.907515\n",
      "max: 0.384358 output: 0.364056\n",
      "max: 0.184520 output: 0.141506\n",
      "max: -0.185583 output: -0.205863\n",
      "max: 1.878006 output: 1.819072\n",
      "max: 0.818198 output: 0.815126\n",
      "max: 0.072115 output: 0.435097\n",
      "max: 1.914209 output: 1.873126\n"
     ]
    }
   ],
   "source": [
    "for(TI i=0; i < 10; i++){\n",
    "    rlt::randn(device, input_mlp, rng);\n",
    "    rlt::forward(device, model, input_mlp);\n",
    "    std::cout << \"max: \" << rlt::max(device, input_mlp) << \" output: \" << rlt::get(model.output_layer.output, 0, 0) << std::endl;\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1f13fabd-a64d-4696-a193-e3fbc163484c",
   "metadata": {},
   "source": [
    "If the values are not close the model might need some more training iterations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++17",
   "language": "C++17",
   "name": "xcpp17"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
